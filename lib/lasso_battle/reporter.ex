defmodule Lasso.Battle.Reporter do
  @moduledoc """
  Generates battle test reports in various formats.

  Supports:
  - JSON (structured data for analysis)
  - Markdown (human-readable summaries)
  - HTML (Phase 2 - interactive reports with charts)
  """

  require Logger

  @doc """
  Saves test results as JSON.

  ## Example

      Reporter.save_json(result, "priv/battle_results/")
      # => Creates priv/battle_results/2025-09-30T14-32-15_abc123.json
  """
  def save_json(result, output_dir) do
    ensure_output_dir(output_dir)

    filename = "#{result.report_id}.json"
    path = Path.join(output_dir, filename)

    json_data = %{
      report_id: result.report_id,
      scenario: %{
        name: result.scenario.name,
        duration_ms: result.duration_ms
      },
      passed: result.passed?,
      analysis: result.analysis,
      slo_results: result.slo_results,
      context: result.context,
      generated_at: DateTime.utc_now() |> DateTime.to_iso8601()
    }

    case Jason.encode(json_data, pretty: true) do
      {:ok, json} ->
        File.write!(path, json)
        Logger.info("Saved JSON report: #{path}")
        {:ok, path}

      {:error, reason} ->
        Logger.error("Failed to encode JSON: #{inspect(reason)}")
        {:error, reason}
    end
  end

  @doc """
  Saves test results as Markdown.

  ## Example

      Reporter.save_markdown(result, "priv/battle_results/")
      # => Creates priv/battle_results/2025-09-30T14-32-15_abc123.md
  """
  def save_markdown(result, output_dir) do
    ensure_output_dir(output_dir)

    filename = "#{result.report_id}.md"
    path = Path.join(output_dir, filename)

    markdown = generate_markdown(result)

    File.write!(path, markdown)
    Logger.info("Saved Markdown report: #{path}")

    # Also save as "latest.md" for easy access
    latest_path = Path.join(output_dir, "latest.md")
    File.write!(latest_path, markdown)

    {:ok, path}
  end

  @doc """
  Saves test results as HTML (Phase 2).
  """
  def save_html(result, output_dir) do
    # Phase 2: Generate interactive HTML with charts
    Logger.info("HTML reports (Phase 2)")
    {:ok, "phase2"}
  end

  @doc """
  Saves current results as baseline for regression testing.
  """
  def save_baseline(result) do
    path = "priv/battle_baseline.json"

    baseline_data = %{
      report_id: result.report_id,
      scenario_name: result.scenario.name,
      analysis: result.analysis,
      saved_at: DateTime.utc_now() |> DateTime.to_iso8601()
    }

    case Jason.encode(baseline_data, pretty: true) do
      {:ok, json} ->
        File.write!(path, json)
        Logger.info("Saved baseline: #{path}")
        {:ok, path}

      {:error, reason} ->
        Logger.error("Failed to save baseline: #{inspect(reason)}")
        {:error, reason}
    end
  end

  @doc """
  Compares current results to baseline.
  """
  def compare_to_baseline(result) do
    baseline_path = "priv/battle_baseline.json"

    case File.read(baseline_path) do
      {:ok, content} ->
        {:ok, baseline} = Jason.decode(content, keys: :atoms)
        comparison = compute_comparison(result.analysis, baseline.analysis)
        {:ok, comparison}

      {:error, :enoent} ->
        {:error, :no_baseline}

      {:error, reason} ->
        {:error, reason}
    end
  end

  # Private helpers

  defp ensure_output_dir(dir) do
    File.mkdir_p!(dir)
  end

  defp generate_markdown(result) do
    """
    # Battle Test Report: #{result.scenario.name}

    **Date:** #{DateTime.utc_now() |> DateTime.to_string()}
    **Duration:** #{format_duration(result.duration_ms)}
    **Result:** #{if result.passed?, do: "✅ PASS", else: "❌ FAIL"}

    ## SLO Compliance

    #{format_slo_table(result.slo_results)}

    ## Performance Summary

    #{format_performance_summary(result.analysis)}

    ## Raw Data

    - Report ID: `#{result.report_id}`
    - JSON Report: `#{result.report_id}.json`

    ---

    Generated by Lasso RPC Battle Testing Framework
    """
  end

  defp format_slo_table(slo_results) do
    if map_size(slo_results) == 0 do
      "_No SLOs defined_"
    else
      header = "| Metric | Required | Actual | Status |\n|--------|----------|--------|--------|"

      rows =
        Enum.map_join(slo_results, "\n", fn {key, result} ->
          status = if result.passed?, do: "✅", else: "❌"

          "| #{format_metric_name(key)} | #{format_slo_value(result.required)} | #{format_slo_value(result.actual)} | #{status} |"
        end)

      header <> "\n" <> rows
    end
  end

  defp format_performance_summary(analysis) do
    requests = Map.get(analysis, :requests, %{})
    cb = Map.get(analysis, :circuit_breaker, %{})
    system = Map.get(analysis, :system, %{})
    by_transport = Map.get(analysis, :requests_by_transport, %{})

    transport_breakdown =
      if by_transport && (by_transport.http || by_transport.ws) do
        format_transport_breakdown(by_transport)
      else
        ""
      end

    """
    ### HTTP Requests

    - Total: #{Map.get(requests, :total, 0)}
    - Successes: #{Map.get(requests, :successes, 0)}
    - Failures: #{Map.get(requests, :failures, 0)}
    - Success Rate: #{format_percent(Map.get(requests, :success_rate, 0.0))}
    - P50 Latency: #{Map.get(requests, :p50_latency_ms, 0)}ms
    - P95 Latency: #{Map.get(requests, :p95_latency_ms, 0)}ms
    - P99 Latency: #{Map.get(requests, :p99_latency_ms, 0)}ms
    #{transport_breakdown}
    ### Circuit Breakers

    - State Changes: #{Map.get(cb, :state_changes, 0)}
    - Opens: #{Map.get(cb, :opens, 0)}
    - Closes: #{Map.get(cb, :closes, 0)}
    - Time Open: #{Map.get(cb, :time_open_ms, 0)}ms

    ### System Health

    - Peak Memory: #{format_mb(Map.get(system, :peak_memory_mb, 0))}
    - Avg Memory: #{format_mb(Map.get(system, :avg_memory_mb, 0))}
    - Peak Processes: #{Map.get(system, :peak_processes, 0)}
    - Avg Processes: #{Map.get(system, :avg_processes, 0) |> trunc()}
    """
  end

  defp format_transport_breakdown(by_transport) do
    http = by_transport[:http]
    ws = by_transport[:ws]

    http_section =
      if http && http.total > 0 do
        provider_dist_text = format_provider_distribution(http.provider_distribution)

        """

        #### HTTP Transport to Upstream

        - Total: #{http.total}
        - Successes: #{http.successes}
        - Failures: #{http.failures}
        - Success Rate: #{format_percent(http.success_rate)}
        - P50 Latency: #{http.p50_latency_ms}ms
        - P95 Latency: #{http.p95_latency_ms}ms
        - P99 Latency: #{http.p99_latency_ms}ms
        - Min: #{http.min_latency_ms}ms | Max: #{http.max_latency_ms}ms | Avg: #{Float.round(http.avg_latency_ms, 2)}ms
        - **Std Dev:** #{Float.round(http.stddev_latency_ms, 2)}ms
        - **95% CI:** [#{Float.round(http.ci_95_lower_ms, 2)}ms, #{Float.round(http.ci_95_upper_ms, 2)}ms]
        #{provider_dist_text}
        """
      else
        ""
      end

    ws_section =
      if ws && ws.total > 0 do
        provider_dist_text = format_provider_distribution(ws.provider_distribution)

        """

        #### WebSocket Transport to Upstream

        - Total: #{ws.total}
        - Successes: #{ws.successes}
        - Failures: #{ws.failures}
        - Success Rate: #{format_percent(ws.success_rate)}
        - P50 Latency: #{ws.p50_latency_ms}ms
        - P95 Latency: #{ws.p95_latency_ms}ms
        - P99 Latency: #{ws.p99_latency_ms}ms
        - Min: #{ws.min_latency_ms}ms | Max: #{ws.max_latency_ms}ms | Avg: #{Float.round(ws.avg_latency_ms, 2)}ms
        - **Std Dev:** #{Float.round(ws.stddev_latency_ms, 2)}ms
        - **95% CI:** [#{Float.round(ws.ci_95_lower_ms, 2)}ms, #{Float.round(ws.ci_95_upper_ms, 2)}ms]
        #{provider_dist_text}
        """
      else
        ""
      end

    comparison =
      if http && ws && http.total > 0 && ws.total > 0 do
        ws_faster_p50 = http.p50_latency_ms - ws.p50_latency_ms
        ws_faster_p95 = http.p95_latency_ms - ws.p95_latency_ms
        ws_faster_p99 = http.p99_latency_ms - ws.p99_latency_ms

        # Statistical significance test
        stat_test = by_transport[:statistical_comparison]

        stat_section =
          if stat_test do
            """

            **Statistical Significance Test (Mann-Whitney U):**
            - U-statistic: #{stat_test.u_statistic}
            - Z-score: #{stat_test.z_score}
            - p-value: #{stat_test.p_value}
            - #{stat_test.interpretation}
            """
          else
            ""
          end

        """

        #### Transport Comparison

        - **P50:** WebSocket is #{format_latency_diff(ws_faster_p50)} HTTP
        - **P95:** WebSocket is #{format_latency_diff(ws_faster_p95)} HTTP
        - **P99:** WebSocket is #{format_latency_diff(ws_faster_p99)} HTTP
        #{stat_section}
        """
      else
        ""
      end

    http_section <> ws_section <> comparison
  end

  defp format_latency_diff(diff) when diff > 0 and is_float(diff),
    do: "#{Float.round(diff, 2)}ms faster than"

  defp format_latency_diff(diff) when diff > 0 and is_integer(diff),
    do: "#{diff}ms faster than"

  defp format_latency_diff(diff) when diff < 0 and is_float(diff),
    do: "#{Float.round(abs(diff), 2)}ms slower than"

  defp format_latency_diff(diff) when diff < 0 and is_integer(diff),
    do: "#{abs(diff)}ms slower than"

  defp format_latency_diff(_), do: "same as"

  defp format_provider_distribution(%{total: 0}), do: ""

  defp format_provider_distribution(%{by_provider: by_provider, balance_score: balance_score}) do
    if map_size(by_provider) == 0 do
      ""
    else
      provider_lines =
        by_provider
        |> Enum.sort_by(fn {_, %{count: c}} -> c end, :desc)
        |> Enum.map(fn {provider, %{count: count, percentage: pct}} ->
          "  - #{provider}: #{count} (#{Float.round(pct, 1)}%)"
        end)
        |> Enum.join("\n")

      balance_emoji =
        cond do
          balance_score >= 0.9 -> "✅"
          balance_score >= 0.7 -> "⚠️"
          true -> "❌"
        end

      """
      - **Provider Distribution (Balance Score: #{balance_score} #{balance_emoji}):**
      #{provider_lines}
      """
    end
  end

  defp format_provider_distribution(_), do: ""

  defp format_metric_name(:success_rate), do: "Success Rate"
  defp format_metric_name(:p95_latency_ms), do: "P95 Latency"
  defp format_metric_name(:p99_latency_ms), do: "P99 Latency"
  defp format_metric_name(:max_failover_latency_ms), do: "Max Failover Latency"
  defp format_metric_name(:max_memory_mb), do: "Max Memory"
  defp format_metric_name(:subscription_uptime), do: "Subscription Uptime"
  defp format_metric_name(:max_duplicate_rate), do: "Max Duplicate Rate"
  defp format_metric_name(other), do: to_string(other)

  defp format_slo_value(val) when is_float(val) do
    cond do
      val >= 0 and val <= 1 -> format_percent(val)
      true -> "#{Float.round(val, 2)}"
    end
  end

  defp format_slo_value(val) when is_integer(val), do: "#{val}ms"
  defp format_slo_value(nil), do: "N/A"
  defp format_slo_value(val), do: inspect(val)

  defp format_percent(rate) when is_float(rate) do
    "#{Float.round(rate * 100, 2)}%"
  end

  defp format_percent(_), do: "N/A"

  defp format_mb(mb) when is_float(mb), do: "#{Float.round(mb, 2)}MB"
  defp format_mb(mb) when is_integer(mb), do: "#{mb}MB"
  defp format_mb(_), do: "N/A"

  defp format_duration(ms) when is_integer(ms) do
    cond do
      ms < 1000 -> "#{ms}ms"
      ms < 60_000 -> "#{Float.round(ms / 1000, 1)}s"
      true -> "#{Float.round(ms / 60_000, 1)}min"
    end
  end

  defp compute_comparison(current, baseline) do
    # Phase 2: Detailed regression analysis
    %{
      success_rate_delta:
        calculate_delta(current.requests.success_rate, baseline.requests.success_rate),
      p95_latency_delta:
        calculate_delta(current.requests.p95_latency_ms, baseline.requests.p95_latency_ms)
    }
  end

  defp calculate_delta(current, baseline) when is_number(current) and is_number(baseline) do
    ((current - baseline) / baseline * 100) |> Float.round(2)
  end

  defp calculate_delta(_, _), do: nil
end
