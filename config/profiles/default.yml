---
name: Lasso Public
slug: default
type: standard
default_rps_limit: 100
default_burst_limit: 500
---
# ════════════════════════════════════════════════════════════════════════════
# LASSO RPC PROFILE CONFIGURATION
# ════════════════════════════════════════════════════════════════════════════
#
# This file is the canonical reference for all profile configuration options.
#
# ## Format
# - Frontmatter (above): Profile metadata (name, slug, type, rate limits)
# - Body (below): Per-chain configurations with providers and settings
#
# ## Environment Variables
# - Provider URLs support `${ENV_VAR}` substitution
# - Lasso will crash at startup if placeholders remain unresolved
#
# ## Profile Metadata Fields
# - `name` (string, required): Display name for the profile
# - `slug` (string, required): Must match filename (e.g., default.yml → slug: default)
# - `type` (enum, required): free | standard | premium | byok
# - `default_rps_limit` (int, required): Default per-client requests/second
# - `default_burst_limit` (int, required): Default burst capacity
#
# ════════════════════════════════════════════════════════════════════════════

chains:
  # ──────────────────────────────────────────────────────────────────────────
  # ETHEREUM MAINNET
  # ──────────────────────────────────────────────────────────────────────────
  ethereum:
    # Chain ID (EIP-155)
    # Type: integer | Required
    chain_id: 1

    # Display name
    # Type: string | Optional | Default: <chain_key>
    name: "Ethereum Mainnet"

    # ┌────────────────────────────────────────────────────────────────────────
    # │ HEALTH MONITORING
    # │ Controls provider health checks and lag detection thresholds.
    # └────────────────────────────────────────────────────────────────────────
    monitoring:
      # How often to probe providers with eth_chainId health checks.
      # Recommendation: ~1x block time for L1, ~1.5x block time for L2.
      # Type: integer (milliseconds) | Default: 12000 | Range: 1000-60000
      probe_interval_ms: 12000

      # Block lag threshold before marking a provider as degraded.
      # This triggers logging/alerting but does NOT exclude from selection.
      # (See selection.max_lag_blocks for filtering threshold)
      # Type: integer (blocks) | Default: 3
      lag_alert_threshold_blocks: 5

    # ┌────────────────────────────────────────────────────────────────────────
    # │ PROVIDER SELECTION
    # │ Controls which providers are eligible for request routing.
    # └────────────────────────────────────────────────────────────────────────
    selection:
      # Maximum block lag before excluding a provider from selection.
      # Providers further behind than this are not considered for routing.
      # L1 recommendation: 1-2 blocks | L2 recommendation: 3-5 blocks
      # Type: integer (blocks) | Default: nil (no filtering)
      max_lag_blocks: 1

      # Block age threshold for archival data filtering.
      # Requests for blocks older than this are routed only to archival providers.
      # L1 recommendation: 1000 blocks (~3.3 hours on Ethereum with 12s blocks)
      # Type: integer (blocks) | Default: 1000
      # archival_threshold: 1000

    # ┌────────────────────────────────────────────────────────────────────────
    # │ WEBSOCKET SUBSCRIPTIONS
    # │ Controls WebSocket behavior for newHeads and subscription failover.
    # └────────────────────────────────────────────────────────────────────────
    websocket:
      # Subscribe to newHeads for real-time block tracking.
      # Enables lag detection and subscription gap-filling on failover.
      # Can be overridden per-provider via provider.subscribe_new_heads.
      # Type: boolean | Default: true
      subscribe_new_heads: true

      # How long without a newHeads event before considering subscription stale.
      # Should be > 2-3x expected block time to avoid false positives.
      # Type: integer (milliseconds) | Default: 42000 | Range: 5000-120000
      new_heads_timeout_ms: 35000

      # Backfill configuration: When a subscription fails over to another provider,
      # fetch missed blocks via HTTP to ensure no data loss during the transition.
      # Example: Provider A dies at block 1002, Provider B starts at 1006.
      # Backfill fetches blocks 1003-1005 via HTTP to fill the gap.
      failover:
        # Maximum missed blocks to fetch during failover.
        # Limits gap-filling scope to prevent excessive HTTP requests on long outages.
        # If gap exceeds this limit, backfill is skipped and gap is reported.
        # Type: integer (blocks) | Default: 100 | Range: 1-1000
        max_backfill_blocks: 100

        # Timeout for fetching missed blocks via HTTP.
        # Type: integer (milliseconds) | Default: 30000 | Range: 5000-120000
        backfill_timeout_ms: 30000

    # ┌────────────────────────────────────────────────────────────────────────
    # │ UI TOPOLOGY (Dashboard visualization only)
    # │ Metadata used by the network topology component for layout/styling.
    # └────────────────────────────────────────────────────────────────────────
    ui-topology:
      color: "#627EEA" # Hex color for dashboard
      size: xl # sm | md | lg | xl

    # ┌────────────────────────────────────────────────────────────────────────
    # │ PROVIDERS
    # │ Upstream RPC endpoints with priority-based routing.
    # └────────────────────────────────────────────────────────────────────────
    providers:
      # ──── Provider Entry ────

      # subscribe_new_heads: true
      # Override chain-level subscribe_new_heads setting
      # Type: boolean | Default: nil (uses chain setting)

      # supports_archival: true
      # Whether provider supports historical/archival data queries
      # Set to false for providers with limited archive depth
      # Type: boolean | Default: nil (treated as true for backward compatibility)

      # adapter_config:
      #   # Provider-specific adapter tuning (optional, advanced)
      #   # Fields vary by provider type - see docs/ADAPTERS.md
      #   # Common examples:
      #   max_block_range: 10000        # eth_getLogs block range limit
      #   eth_get_logs_block_range: 10  # Alchemy-specific override

      - id: "ethereum_drpc"
        name: "dRPC Ethereum"
        priority: 2
        url: "https://eth.drpc.org"
        ws_url: "wss://eth.drpc.org"

      - id: "ethereum_publicnode"
        name: "PublicNode Ethereum"
        priority: 3
        url: "https://ethereum-rpc.publicnode.com"
        ws_url: "wss://ethereum-rpc.publicnode.com"

      # Temporarily disable: Uses a proxied backend with some nodes using PHE and some not (create inconsistency in log responses)
      # - id: "ethereum_merkle"
      #   name: "Merkle Ethereum"
      #   priority: 4
      #   url: "https://eth.merkle.io"

      - id: "0xrpc_ethereum"
        name: "0xRPC Ethereum"
        priority: 5
        url: "https://0xrpc.io/eth"
        ws_url: "wss://0xrpc.io/eth"

      - id: "ethereum_lava"
        name: "Lava Ethereum"
        priority: 6
        url: "https://eth1.lava.build"

      - id: "ethereum_mevblocker"
        name: "MEV Blocker"
        priority: 7
        url: "https://rpc.mevblocker.io"

      # NOTE: Providers removed due to lack of historical eth_getLogs support
      # These providers can serve recent blocks but return null for historical
      # logs, which breaks indexers like Ponder, Subgraph, etc.
      # - BlockRazor (eth.blockrazor.xyz)
      # - bloXroute Virginia (virginia.rpc.blxrbdn.com)

  # ──────────────────────────────────────────────────────────────────────────
  # BASE MAINNET (L2)
  # ──────────────────────────────────────────────────────────────────────────
  base:
    chain_id: 8453
    name: "Base Mainnet"

    monitoring:
      # L2s have faster block times - probe more frequently
      probe_interval_ms: 3000 # 3s = ~1.5x block time (2s blocks)
      lag_alert_threshold_blocks: 5

    selection:
      # More tolerant for L2 due to faster block times
      max_lag_blocks: 5
      # L2 recommendation: 1800 blocks (~1 hour on Base with 2s blocks)
      archival_threshold: 1800

    websocket:
      subscribe_new_heads: true
      # Shorter timeout for L2 (faster blocks)
      new_heads_timeout_ms: 20000

      # Backfill missed blocks during subscription failover
      failover:
        max_backfill_blocks: 100
        backfill_timeout_ms: 30000

    ui-topology:
      color: "#0052FF"
      size: md

    providers:
      - id: "base_llamarpc"
        name: "LlamaRPC Base"
        priority: 2
        url: "https://base.llamarpc.com"
        ws_url: "wss://base.llamarpc.com"

      - id: "base_official"
        name: "Base Official"
        priority: 3
        url: "https://mainnet.base.org"

      - id: "base_publicnode"
        name: "PublicNode Base"
        priority: 4
        url: "https://base.publicnode.com"
        ws_url: "wss://base.publicnode.com"

      - id: "base_drpc"
        name: "dRPC Base"
        priority: 5
        url: "https://base.drpc.org"
        ws_url: "wss://base.drpc.org"

      - id: "base_lava"
        name: "Lava Base"
        priority: 6
        url: "https://base.lava.build"

  # ──────────────────────────────────────────────────────────────────────────
  # ARBITRUM ONE (L2)
  # ──────────────────────────────────────────────────────────────────────────
  arbitrum:
    chain_id: 42161
    name: "Arbitrum One"

    monitoring:
      # Arbitrum has very fast blocks (~0.25s) - probe frequently
      probe_interval_ms: 2000 # 2s = ~8x block time
      lag_alert_threshold_blocks: 15

    selection:
      # More tolerant for L2 due to very fast block times
      max_lag_blocks: 10
      # L2 recommendation: 14400 blocks (~1 hour on Arbitrum with 0.25s blocks)
      archival_threshold: 14400

    websocket:
      subscribe_new_heads: true
      # Shorter timeout for fast L2
      new_heads_timeout_ms: 15000

      # Backfill missed blocks during subscription failover
      failover:
        max_backfill_blocks: 100
        backfill_timeout_ms: 30000

    ui-topology:
      color: "#28A0F0"
      size: md

    providers:
      - id: "arbitrum_drpc"
        name: "dRPC Arbitrum"
        priority: 2
        url: "https://arbitrum.drpc.org"
        ws_url: "wss://arbitrum.drpc.org"

      - id: "arbitrum_lava"
        name: "Lava Arbitrum"
        priority: 3
        url: "https://arb1.lava.build"

      - id: "arbitrum_publicnode"
        name: "PublicNode Arbitrum"
        priority: 4
        url: "https://arbitrum-one-rpc.publicnode.com"
        ws_url: "wss://arbitrum-one-rpc.publicnode.com"

      - id: "arbitrum_meowrpc"
        name: "Meow RPC Arbitrum"
        priority: 5
        url: "https://arbitrum.meowrpc.com"

      - id: "arbitrum_pocket"
        name: "Pocket Network Arbitrum"
        priority: 6
        url: "https://arb-one.api.pocket.network"
# ════════════════════════════════════════════════════════════════════════════
# CONFIGURATION NOTES
# ════════════════════════════════════════════════════════════════════════════
#
# ## Tuning Guidelines
#
# ### L1 Chains (Ethereum)
# - probe_interval_ms: 12000 (~1x block time)
# - lag_alert_threshold_blocks: 3-5
# - max_lag_blocks: 1-2
# - new_heads_timeout_ms: 35000-42000 (~3x block time)
#
# ### L2 Chains (Base, Optimism, Arbitrum)
# - probe_interval_ms: 3000-4000 (~1.5x block time)
# - lag_alert_threshold_blocks: 5-10
# - max_lag_blocks: 3-5
# - new_heads_timeout_ms: 15000-20000 (~5-10x block time)
#
# ### Testnets
# - Use more lenient thresholds (testnets are less reliable)
# - Increase new_heads_timeout_ms by 50-100%
# - Increase lag_alert_threshold_blocks
#
# ## Provider Adapter Configuration
#
# Some providers have specific limitations or optimizations. Use adapter_config
# to tune provider-specific behavior:
#
# ### Alchemy
#   adapter_config:
#     eth_get_logs_block_range: 10  # Max block range for getLogs
#
# ### 1RPC, dRPC, LlamaRPC, Merkle
#   adapter_config:
#     max_block_range: 10000  # Generic block range limit
#
# See docs/ADAPTERS.md for complete provider-specific configuration options.
#
# ════════════════════════════════════════════════════════════════════════════
